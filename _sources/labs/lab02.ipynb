{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e56de0e5",
   "metadata": {},
   "source": [
    "# Lab 02: Georefereing Location-based Social Media  \n",
    "In this tutorial, we will learn:\n",
    "- How to extract information (e.g., tweets) from location-based social media (e.g., Twitter)\n",
    "- How to identify locational (e.g., place name) information from text-based data (e.g., tweets, newspapers)\n",
    "- How to refer the identified location to metric-based coordinates on the surface of the earcth \n",
    "\n",
    "Several libraries/packages are needed in this tutorial. Use `pip` or `conda` to install them:\n",
    "- [tweepy](https://pypi.org/project/tweepy/): this is library to access the Tweeter API\n",
    "- [spaCy](https://spacy.io/usage): this is the libeary to do natural lanuguage processing \n",
    "- [spacy-dbpedia-spotlight](https://pypi.org/project/spacy-dbpedia-spotlight/): a small library that annotate recognized entities from spaCy to DBpedia enities. \n",
    "- ... \n",
    "\n",
    "## Part 1: Extracting (geo)text from Twitter\n",
    "This part explains how to extract text-based unstructured information from the social media Twitter via its API. Similar pipline can be used to extract information from other types of social media/Web services (e.g., Foursquare, Yelp, Flickr, etc.).  \n",
    "\n",
    "Twitter is a useful data source for studying the social impacts of events and activities. In this part, we are going to learn how to collect Twitter data using its API. Specifically, we are going to focus on geotagged Twitter data.\n",
    "\n",
    "First, Twitter requires the users of Twitter API to be authenticated by the system. One simple approach to obtain such authentication is by registering a Twitter account. This is the approach we are going to take in this tutorial. \n",
    "\n",
    "Go to the website of Twitter: https://twitter.com/ , and click “Sign up” at the upper right corner. You can skip this step if you already have a Twitter account.\n",
    "\n",
    "After you have registered/logged in to your Twitter account, we are going to obtain the keys that are necessary to use Twitter API. Go to https://apps.twitter.com/ , and sign in using the Twitter account you have just created\n",
    "(sometimes the browser will automatically sign you in).\n",
    "\n",
    "After you have signed in, click the button “Create New App”. Then fill in the necessary information to create an APP. Note that you might need to record your phone number in your Twitter account in order to do so. If you don't like it, feel free to remove your phone number from your account after you have done your project. \n",
    "\n",
    "Then you will be directed to a page (see example below) asking you for a name of your App. Give it a name that you want. \n",
    "\n",
    "![Get Keys from Twitter Developer](lab2-fig1.png)\n",
    "\n",
    "Click `Get keys`. It will then generated API Key, API Key Secret, and Bearer Token (see below for an example). Make sure you copy and paste them into a safe place (e.g., a text editor). We need these authentications later. \n",
    "\n",
    "![Authentication Example](lab2-fig2.png)\n",
    "\n",
    "Next, we also need to obtain the Access Token and its key. To do so, go to the `Projects & Apps`--> Select your App. Then click `Keys and tokens`, and then click `Generate` on the right of `Access Token ane Secret` (see below). Again, make sure you record them in a safe place. We need them later. Note that if for some reasons, you lose your tokens and secrets, this page is where you regenerate them. \n",
    "\n",
    "![Access Token Example](lab2-fig3.png)\n",
    "\n",
    "Once you have your Twitter app set-up, you are ready to access tweets in Python. Begin by importing the necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c15456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tweepy as tw\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910ee6a8",
   "metadata": {},
   "source": [
    "To access the Twitter API, you will need four things from the your Twitter App page. These keys are located in your Twitter app settings in the Keys and Access Tokens tab.\n",
    "- api key\n",
    "- api key seceret\n",
    "- access token \n",
    "- access token secret \n",
    "\n",
    "Below I put in my authentications. You should use yours! But remember to not share these with anyone else because these values are specific to your app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53bbb990",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key= '5TX6isrDz92kOC1s7qsTFWq5F'\n",
    "api_key_secret= 'DL4Gw2WLNo2bK538lL5GeNYCtiwlsuYUHOlW8NCSQszK3ac101'\n",
    "access_token= '1582847729486684180-VH5N9AEb2zyyFyLOj5BuD8I9ca0ils'\n",
    "access_token_secret = 'e0cvkxJz9AWvu9dq0Fb48r61vkmIaA1JqLLyhEhms5FGt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee7d8be",
   "metadata": {},
   "source": [
    "With these authentications, we can next build an API variable in Python that build the connection between this Jupyter program and Twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88714009",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tw.OAuthHandler(api_key, api_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbec606e",
   "metadata": {},
   "source": [
    "For example, now we can send tweets using your API access. Note that your tweet needs to be 280 characters or less:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46d582c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Status(_api=<tweepy.api.API object at 0x7fe4200e7700>, _json={'created_at': 'Wed Oct 19 22:55:47 +0000 2022', 'id': 1582868138827296768, 'id_str': '1582868138827296768', 'text': \"Hello Twitter, I'm sending the first message via Python to you! I learnt it from GEOGM0068. #DataScience\", 'truncated': False, 'entities': {'hashtags': [{'text': 'DataScience', 'indices': [92, 104]}], 'symbols': [], 'user_mentions': [], 'urls': []}, 'source': '<a href=\"https://ruizhugeographer.com/\" rel=\"nofollow\">GEOGM0068-Zhu</a>', 'in_reply_to_status_id': None, 'in_reply_to_status_id_str': None, 'in_reply_to_user_id': None, 'in_reply_to_user_id_str': None, 'in_reply_to_screen_name': None, 'user': {'id': 1582847729486684180, 'id_str': '1582847729486684180', 'name': 'Richard Chu', 'screen_name': 'GEOGM0068', 'location': '', 'description': '', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 0, 'friends_count': 1, 'listed_count': 0, 'created_at': 'Wed Oct 19 21:35:04 +0000 2022', 'favourites_count': 0, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 1, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': None, 'profile_background_image_url_https': None, 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1582847827062972431/-8c67lsJ_normal.png', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1582847827062972431/-8c67lsJ_normal.png', 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': True, 'default_profile': True, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none', 'withheld_in_countries': []}, 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'is_quote_status': False, 'retweet_count': 0, 'favorite_count': 0, 'favorited': False, 'retweeted': False, 'lang': 'en'}, created_at=datetime.datetime(2022, 10, 19, 22, 55, 47, tzinfo=datetime.timezone.utc), id=1582868138827296768, id_str='1582868138827296768', text=\"Hello Twitter, I'm sending the first message via Python to you! I learnt it from GEOGM0068. #DataScience\", truncated=False, entities={'hashtags': [{'text': 'DataScience', 'indices': [92, 104]}], 'symbols': [], 'user_mentions': [], 'urls': []}, source='GEOGM0068-Zhu', source_url='https://ruizhugeographer.com/', in_reply_to_status_id=None, in_reply_to_status_id_str=None, in_reply_to_user_id=None, in_reply_to_user_id_str=None, in_reply_to_screen_name=None, author=User(_api=<tweepy.api.API object at 0x7fe4200e7700>, _json={'id': 1582847729486684180, 'id_str': '1582847729486684180', 'name': 'Richard Chu', 'screen_name': 'GEOGM0068', 'location': '', 'description': '', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 0, 'friends_count': 1, 'listed_count': 0, 'created_at': 'Wed Oct 19 21:35:04 +0000 2022', 'favourites_count': 0, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 1, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': None, 'profile_background_image_url_https': None, 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1582847827062972431/-8c67lsJ_normal.png', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1582847827062972431/-8c67lsJ_normal.png', 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': True, 'default_profile': True, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none', 'withheld_in_countries': []}, id=1582847729486684180, id_str='1582847729486684180', name='Richard Chu', screen_name='GEOGM0068', location='', description='', url=None, entities={'description': {'urls': []}}, protected=False, followers_count=0, friends_count=1, listed_count=0, created_at=datetime.datetime(2022, 10, 19, 21, 35, 4, tzinfo=datetime.timezone.utc), favourites_count=0, utc_offset=None, time_zone=None, geo_enabled=False, verified=False, statuses_count=1, lang=None, contributors_enabled=False, is_translator=False, is_translation_enabled=False, profile_background_color='F5F8FA', profile_background_image_url=None, profile_background_image_url_https=None, profile_background_tile=False, profile_image_url='http://pbs.twimg.com/profile_images/1582847827062972431/-8c67lsJ_normal.png', profile_image_url_https='https://pbs.twimg.com/profile_images/1582847827062972431/-8c67lsJ_normal.png', profile_link_color='1DA1F2', profile_sidebar_border_color='C0DEED', profile_sidebar_fill_color='DDEEF6', profile_text_color='333333', profile_use_background_image=True, has_extended_profile=True, default_profile=True, default_profile_image=False, following=False, follow_request_sent=False, notifications=False, translator_type='none', withheld_in_countries=[]), user=User(_api=<tweepy.api.API object at 0x7fe4200e7700>, _json={'id': 1582847729486684180, 'id_str': '1582847729486684180', 'name': 'Richard Chu', 'screen_name': 'GEOGM0068', 'location': '', 'description': '', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 0, 'friends_count': 1, 'listed_count': 0, 'created_at': 'Wed Oct 19 21:35:04 +0000 2022', 'favourites_count': 0, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 1, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': None, 'profile_background_image_url_https': None, 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1582847827062972431/-8c67lsJ_normal.png', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1582847827062972431/-8c67lsJ_normal.png', 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': True, 'default_profile': True, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none', 'withheld_in_countries': []}, id=1582847729486684180, id_str='1582847729486684180', name='Richard Chu', screen_name='GEOGM0068', location='', description='', url=None, entities={'description': {'urls': []}}, protected=False, followers_count=0, friends_count=1, listed_count=0, created_at=datetime.datetime(2022, 10, 19, 21, 35, 4, tzinfo=datetime.timezone.utc), favourites_count=0, utc_offset=None, time_zone=None, geo_enabled=False, verified=False, statuses_count=1, lang=None, contributors_enabled=False, is_translator=False, is_translation_enabled=False, profile_background_color='F5F8FA', profile_background_image_url=None, profile_background_image_url_https=None, profile_background_tile=False, profile_image_url='http://pbs.twimg.com/profile_images/1582847827062972431/-8c67lsJ_normal.png', profile_image_url_https='https://pbs.twimg.com/profile_images/1582847827062972431/-8c67lsJ_normal.png', profile_link_color='1DA1F2', profile_sidebar_border_color='C0DEED', profile_sidebar_fill_color='DDEEF6', profile_text_color='333333', profile_use_background_image=True, has_extended_profile=True, default_profile=True, default_profile_image=False, following=False, follow_request_sent=False, notifications=False, translator_type='none', withheld_in_countries=[]), geo=None, coordinates=None, place=None, contributors=None, is_quote_status=False, retweet_count=0, favorite_count=0, favorited=False, retweeted=False, lang='en')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Post a tweet from Python\n",
    "api.update_status(\"Hello Twitter, I'm sending the first message via Python to you! I learnt it from GEOGM0068. #DataScience\")\n",
    "# Your tweet has been posted!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6746a9",
   "metadata": {},
   "source": [
    "If you go to your Twitter account and check Profile, you will see the tweet being posted! Congrats for your first post via Python!  \n",
    "\n",
    "Note that if you see errors like \"453 - You currently have Essential access which includes access to Twitter API v2 endpoints only. If you need access to this endpoint, you’ll need to apply for Elevated access via the Developer Portal. You can learn more here: https://developer.twitter.com/en/docs/twitter-api/getting-started/about-twitter-api#v2-access-leve\". It means you need to elevate your access. What you need to do is (1). Go to Products --> Twitter API v2; (2). click the tab \"Elevated\" (or \"Academic Research\" if you need it for your dissertation later); (3). Click `Apply`, then file the form (you can choose No for many of the questions). See screenshot below for (1) and (2):\n",
    "\n",
    "![Elevate your access](lab2-fig4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c147af",
   "metadata": {},
   "source": [
    "Next, let's retrieve (search) some tweets that are about `#energycrisis` that are posted since 2022-10-01 in English. There are going to be many posts returned. To make it easy to illustrate and to save some request (note you have a limited number of requests via this API), we only request 5 from the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2ba609fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tweepy.cursor.ItemIterator at 0x7fe4101d0f10>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_words = \"#energycrisis\"\n",
    "tweets = tw.Cursor(api.search_tweets,\n",
    "              q=search_words,\n",
    "              lang=\"en\").items(5)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2521660",
   "metadata": {},
   "source": [
    "Here, you see a  an object that you can iterate (i.e. `ItemIterator`) or loop over to access the data collected. Each item in the iterator has various attributes that you can access to get information about each tweet including:\n",
    "\n",
    "- the text of the tweet\n",
    "- who sent the tweet\n",
    "- the date the tweet was sent\n",
    "\n",
    "and more. The code below loops through the object and save the time of the tweet, the user who posted the tweet, the text of the tweet, as well ast the user location to a pandas `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3ff1cf39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>User</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-19 23:58:43+00:00</td>\n",
       "      <td>wildbluethistle</td>\n",
       "      <td>Lonely inside https://t.co/XFcwiIFhXa #goodmus...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-19 23:58:12+00:00</td>\n",
       "      <td>DrowerR</td>\n",
       "      <td>RT @DaveDavos2: @Bowenchris That will NEVER de...</td>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-19 23:57:48+00:00</td>\n",
       "      <td>grahamtfn</td>\n",
       "      <td>RT @lucycowan83: We're encouraging voluntary o...</td>\n",
       "      <td>Glasgow, Scotland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-19 23:57:16+00:00</td>\n",
       "      <td>SocEntEdinburgh</td>\n",
       "      <td>RT @socialprintandc: For further details on ho...</td>\n",
       "      <td>Edinburgh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-19 23:53:03+00:00</td>\n",
       "      <td>lindakillian</td>\n",
       "      <td>RT @LevittFlisser: High electric bill poised t...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Time             User  \\\n",
       "0 2022-10-19 23:58:43+00:00  wildbluethistle   \n",
       "1 2022-10-19 23:58:12+00:00          DrowerR   \n",
       "2 2022-10-19 23:57:48+00:00        grahamtfn   \n",
       "3 2022-10-19 23:57:16+00:00  SocEntEdinburgh   \n",
       "4 2022-10-19 23:53:03+00:00     lindakillian   \n",
       "\n",
       "                                               Tweet             Location  \n",
       "0  Lonely inside https://t.co/XFcwiIFhXa #goodmus...                       \n",
       "1  RT @DaveDavos2: @Bowenchris That will NEVER de...  Melbourne, Victoria  \n",
       "2  RT @lucycowan83: We're encouraging voluntary o...    Glasgow, Scotland  \n",
       "3  RT @socialprintandc: For further details on ho...            Edinburgh  \n",
       "4  RT @LevittFlisser: High electric bill poised t...                       "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create dataframe\n",
    "columns = ['Time', 'User', 'Tweet', 'Location']\n",
    "\n",
    "data = []\n",
    "for tweet in tweets:\n",
    "    data.append([tweet.created_at, tweet.user.screen_name, tweet.text, tweet.user.location])\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf80d2e",
   "metadata": {},
   "source": [
    "We can further save the dataframe to a local csv file (structured data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "91324fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tweets_example.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93e9afb",
   "metadata": {},
   "source": [
    "Note that there is another way of writing the query to Twitter API, which might be more intuitive to some users. For example, you can replace `tweets = tw.Cursor(api.search_tweets,q=search_words,lang=\"en\").items(5)` to something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e94dcae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>User</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-20 00:00:59+00:00</td>\n",
       "      <td>WhosFibbing</td>\n",
       "      <td>RT @DaveDavos2: @Bowenchris That will NEVER de...</td>\n",
       "      <td>Everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-19 23:59:12+00:00</td>\n",
       "      <td>SocEntEdinburgh</td>\n",
       "      <td>RT @socialprintandc: For further details on ho...</td>\n",
       "      <td>Edinburgh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-19 23:58:43+00:00</td>\n",
       "      <td>wildbluethistle</td>\n",
       "      <td>Lonely inside https://t.co/XFcwiIFhXa #goodmus...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-19 23:58:12+00:00</td>\n",
       "      <td>DrowerR</td>\n",
       "      <td>RT @DaveDavos2: @Bowenchris That will NEVER de...</td>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-19 23:57:48+00:00</td>\n",
       "      <td>grahamtfn</td>\n",
       "      <td>RT @lucycowan83: We're encouraging voluntary o...</td>\n",
       "      <td>Glasgow, Scotland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Time             User  \\\n",
       "0 2022-10-20 00:00:59+00:00      WhosFibbing   \n",
       "1 2022-10-19 23:59:12+00:00  SocEntEdinburgh   \n",
       "2 2022-10-19 23:58:43+00:00  wildbluethistle   \n",
       "3 2022-10-19 23:58:12+00:00          DrowerR   \n",
       "4 2022-10-19 23:57:48+00:00        grahamtfn   \n",
       "\n",
       "                                               Tweet             Location  \n",
       "0  RT @DaveDavos2: @Bowenchris That will NEVER de...           Everywhere  \n",
       "1  RT @socialprintandc: For further details on ho...            Edinburgh  \n",
       "2  Lonely inside https://t.co/XFcwiIFhXa #goodmus...                       \n",
       "3  RT @DaveDavos2: @Bowenchris That will NEVER de...  Melbourne, Victoria  \n",
       "4  RT @lucycowan83: We're encouraging voluntary o...    Glasgow, Scotland  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets2 = api.search_tweets(q=search_words,lang=\"en\", count=\"5\")\n",
    "data2 = []\n",
    "for tweet2 in tweets2:\n",
    "    data2.append([tweet2.created_at, tweet2.user.screen_name, tweet2.text, tweet2.user.location])\n",
    "df2 = pd.DataFrame(data2, columns=columns)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019467ba",
   "metadata": {},
   "source": [
    "To learn more about the key function `search_tweets()`, check its webpage [here](https://docs.tweepy.org/en/stable/api.html#tweepy.API.search_tweets). Please try yourself to set up some other parameters to see what you can get. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d1b299",
   "metadata": {},
   "source": [
    "## Part 2: Basic Natural Language Processing and Geoparsing\n",
    "To extract places (or other categories) from text-based (unstructured) data, we need to do some basic Natural Language Processing (NLP), such as tokenization and Part-of-Speech analysis. All these operations can be done through the library `spaCy`. \n",
    "\n",
    "Ideally, you can use the tweets you got from Part 1 to do the experiment. But since sometimes the tweets you get might be very heterogenous and noisy, here we use a clean example (you can also get it from some long news online) to show how to use `spaCy` in order to make sure all the knowledge points are covered in one example. \n",
    "\n",
    "First make sure you have intsalled and imported `spaCy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "33fa1971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881eadbe",
   "metadata": {},
   "source": [
    "`spaCy` comes with pretrained NLP models that can perform most common NLP tasks, such as tokenization, parts of speech (POS) tagging, named entity recognition (NER), transforming to word vectors etc.\n",
    "\n",
    "If you are dealing with a particular language, you can load the spacy model specific to the language using spacy.load() function. For example, we want to load the English version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e5c0bdcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7fe3f00d2fd0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load small english model: https://spacy.io/models\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd298ca0",
   "metadata": {},
   "source": [
    "This returns a Language object that comes ready with multiple built-in capabilities. \n",
    "\n",
    "Now let's say you have your text data in a string. What can be done to understand the structure of the text?\n",
    "\n",
    "First, call the loaded nlp object on the text. It should return a processed Doc object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "700d6d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse text through the `nlp` model\n",
    "my_text = \"\"\"The economic situation of the country is on edge , as the stock \n",
    "market crashed causing loss of millions. Citizens who had their main investment \n",
    "in the share-market are facing a great loss. Many companies might lay off \n",
    "thousands of people to reduce labor cost\"\"\"\n",
    "\n",
    "my_doc = nlp(my_text)\n",
    "type(my_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b3823c",
   "metadata": {},
   "source": [
    "Hmmm, it is a  Doc object. But wait, what exactly is a Doc object?\n",
    "\n",
    "It is a sequence of tokens that contains not just the original text but all the results produced by the spaCy model after processing the text. Useful information such as the lemma of the text, whether it is a stop word or not, named entities, the word vector of the text and so on are pre-computed and readily stored in the Doc object.\n",
    "\n",
    "So first, what is a token? \n",
    "\n",
    "As you have learnt from the lecture. Tokens are individual text entities that make up the text. Typically a token can be the words, punctuation, spaces, etc. Tokenization is the process of converting a text into smaller sub-texts, based on certain predefined rules. For example, sentences are tokenized to words (and punctuation optionally). And paragraphs into sentences, depending on the context.\n",
    "\n",
    "Each token in `spacy` has different attributes that tell us a great deal of information.\n",
    "\n",
    "Let’s see the token texts on `my_doc`. The string which the token represents can be accessed through the `token.text` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2d6313ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "economic\n",
      "situation\n",
      "of\n",
      "the\n",
      "country\n",
      "is\n",
      "on\n",
      "edge\n",
      ",\n",
      "as\n",
      "the\n",
      "stock\n",
      "\n",
      "\n",
      "market\n",
      "crashed\n",
      "causing\n",
      "loss\n",
      "of\n",
      "millions\n",
      ".\n",
      "Citizens\n",
      "who\n",
      "had\n",
      "their\n",
      "main\n",
      "investment\n",
      "\n",
      "\n",
      "in\n",
      "the\n",
      "share\n",
      "-\n",
      "market\n",
      "are\n",
      "facing\n",
      "a\n",
      "great\n",
      "loss\n",
      ".\n",
      "Many\n",
      "companies\n",
      "might\n",
      "lay\n",
      "off\n",
      "\n",
      "\n",
      "thousands\n",
      "of\n",
      "people\n",
      "to\n",
      "reduce\n",
      "labor\n",
      "cost\n"
     ]
    }
   ],
   "source": [
    "# Printing the tokens of a doc\n",
    "for token in my_doc:\n",
    "  print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4d5ae",
   "metadata": {},
   "source": [
    "The above tokens contain punctuation and common words like “a”, ” the”, “was”, etc. These do not add any value to the meaning of your text. They are called stop words. We can clean it up.\n",
    "\n",
    "The type of tokens will allow us to clean those noisy tokens such as stop word, punctuation, and space. First, we show whether a token is stop/punctuation or not, and then we use this information to remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "92ccf80b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The -- True --- False\n",
      "economic -- False --- False\n",
      "situation -- False --- False\n",
      "of -- True --- False\n",
      "the -- True --- False\n",
      "country -- False --- False\n",
      "is -- True --- False\n",
      "on -- True --- False\n",
      "edge -- False --- False\n",
      ", -- False --- True\n",
      "as -- True --- False\n",
      "the -- True --- False\n",
      "stock -- False --- False\n",
      "\n",
      " -- False --- False\n",
      "market -- False --- False\n",
      "crashed -- False --- False\n",
      "causing -- False --- False\n",
      "loss -- False --- False\n",
      "of -- True --- False\n",
      "millions -- False --- False\n",
      ". -- False --- True\n",
      "Citizens -- False --- False\n",
      "who -- True --- False\n",
      "had -- True --- False\n",
      "their -- True --- False\n",
      "main -- False --- False\n",
      "investment -- False --- False\n",
      "\n",
      " -- False --- False\n",
      "in -- True --- False\n",
      "the -- True --- False\n",
      "share -- False --- False\n",
      "- -- False --- True\n",
      "market -- False --- False\n",
      "are -- True --- False\n",
      "facing -- False --- False\n",
      "a -- True --- False\n",
      "great -- False --- False\n",
      "loss -- False --- False\n",
      ". -- False --- True\n",
      "Many -- True --- False\n",
      "companies -- False --- False\n",
      "might -- True --- False\n",
      "lay -- False --- False\n",
      "off -- True --- False\n",
      "\n",
      " -- False --- False\n",
      "thousands -- False --- False\n",
      "of -- True --- False\n",
      "people -- False --- False\n",
      "to -- True --- False\n",
      "reduce -- False --- False\n",
      "labor -- False --- False\n",
      "cost -- False --- False\n"
     ]
    }
   ],
   "source": [
    "# Printing tokens and boolean values stored in different attributes\n",
    "for token in my_doc:\n",
    "  print(token.text,'--',token.is_stop,'---',token.is_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "33552ee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "economic\n",
      "situation\n",
      "country\n",
      "edge\n",
      "stock\n",
      "market\n",
      "crashed\n",
      "causing\n",
      "loss\n",
      "millions\n",
      "Citizens\n",
      "main\n",
      "investment\n",
      "share\n",
      "market\n",
      "facing\n",
      "great\n",
      "loss\n",
      "companies\n",
      "lay\n",
      "thousands\n",
      "people\n",
      "reduce\n",
      "labor\n",
      "cost\n"
     ]
    }
   ],
   "source": [
    "# Removing StopWords and punctuations\n",
    "my_doc_cleaned = [token for token in my_doc if not token.is_stop and not token.is_punct and not token.is_space]\n",
    "\n",
    "for token in my_doc_cleaned:\n",
    "  print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a25b32",
   "metadata": {},
   "source": [
    "To get the POS tagging of your text, you use code like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6d82abb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "economic ----  ADJ\n",
      "situation ----  NOUN\n",
      "country ----  NOUN\n",
      "edge ----  NOUN\n",
      "stock ----  NOUN\n",
      "market ----  NOUN\n",
      "crashed ----  VERB\n",
      "causing ----  VERB\n",
      "loss ----  NOUN\n",
      "millions ----  NOUN\n",
      "Citizens ----  NOUN\n",
      "main ----  ADJ\n",
      "investment ----  NOUN\n",
      "share ----  NOUN\n",
      "market ----  NOUN\n",
      "facing ----  VERB\n",
      "great ----  ADJ\n",
      "loss ----  NOUN\n",
      "companies ----  NOUN\n",
      "lay ----  VERB\n",
      "thousands ----  NOUN\n",
      "people ----  NOUN\n",
      "reduce ----  VERB\n",
      "labor ----  NOUN\n",
      "cost ----  NOUN\n"
     ]
    }
   ],
   "source": [
    "for token in my_doc_cleaned:\n",
    "  print(token.text,'---- ',token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddd50be",
   "metadata": {},
   "source": [
    "You will see each word (tokenization) now is associated with a POS tag, whether it is a Noun, a Adj, a Verb, or so on ... POS often can help us disambiguate the meaning of words (or places in GIR). \n",
    "\n",
    "Btw, if you don't know what \"ADJ\" means, you can use code like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dcb9de31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adjective'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('ADJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b35e4e",
   "metadata": {},
   "source": [
    "You can also use `spaCy` to do some Named Entity Recognition (including place name identification or geoparsing). For you instance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1f730cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony Stark ---  PERSON\n",
      "StarkEnterprises ---  ORG\n",
      "Emily Clark ---  PERSON\n",
      "Microsoft ---  ORG\n",
      "Manchester ---  GPE\n",
      "Bible ---  WORK_OF_ART\n",
      "French ---  NORP\n"
     ]
    }
   ],
   "source": [
    "text='Tony Stark owns the company StarkEnterprises . Emily Clark works at Microsoft and lives in Manchester. She loves to read the Bible and learn French'\n",
    "doc=nlp(text)\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(entity.text,'--- ',entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f608113d",
   "metadata": {},
   "source": [
    "What is \"GPE\"? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ada5e86c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97f544a",
   "metadata": {},
   "source": [
    "spaCy also provides special visualization for NER through displacy. Using displacy.render() function, you can set the style=ent to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e00b89c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tony Stark\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " owns the company \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    StarkEnterprises\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " . \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Emily Clark\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " works at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Microsoft\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and lives in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Manchester\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ". She loves to read the \n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bible\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       " and learn \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    French\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using displacy for visualizing NER\n",
    "from spacy import displacy\n",
    "displacy.render(doc,style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0617e5ec",
   "metadata": {},
   "source": [
    "So far, you have learnt the basics of retrieving information from social media like Twitter, as well as basic NLP operations and named entity recognition (geoparsing is part of it). I suggest you to play with what you have learnt so far by using new data to experiment these functions, changing the parameters of function, combining these skills with what you have learn in Tutorial 1 (e.g., geopandas), etc.\n",
    "\n",
    "Next week, we will try more cool libraries and examples related to geoparsing and geocoding. \n",
    "\n",
    "TBC next week ... "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
